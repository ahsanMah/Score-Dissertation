\chapter{Background and Prior Work}
\label{ch:background}

\section{Score Matching}
Recall that a score is defined as the gradient of the log probability density, with respect to the data. Conceptually, a score is a vector field that points in the direction where the log density grows the most. 
\cite{hyvarinen2005} introduced score matching as a means of computing the parameters of an unnormalized probability density model. The authors proved the remarkable property that learning the score involves the gradient of the score function itself as shown in Equation~\ref{eq:implicit_sm}. I will follow the naming scheme used in \cite{vincent2011connection} and call this objective as Implicit Score Matching.

\begin{align}
\label{eq:implicit_sm}
    J_{ISM}(\theta) &= \mathbb{E}_{x \sim p(x)} \frac{1}{2} \s{ \norm{s_\theta(x) - \nabla_x \log p(x) }^2 } \\
    &= \mathbb{E}_{x \sim p(x)} \s{ \norm{s_\theta(x)}^2 + \sum^{d}_{i=1}{\partial{x_{i}} s(x_{i}) } }
\end{align}


\subsection*{Denoising Score Matching}

\cite{vincent2011connection} formalized a connection between denoising autoencoders and score matching, and proposed the denoising score matching (DSM) objective.

The authors noted how \cite{hyvarinen2005} had suggested the possibility of an alternate score matching objective; one that was based on regressing against the data gradients of a Parzen window density estimator. This so-called Explicit Score Matching objective is shown in \eqnref{eq:explicit_sm}.

\begin{align}
\label{eq:explicit_sm}
    J_{ESM}(\theta) &= \mathbb{E}_{x \sim q_{\sigma}(x)} \frac{1}{2} \s{ \norm{s_\theta(x) - \nabla_x \log q_{\sigma}(x) }^2 }
\end{align}

\cite{vincent2011connection} proved that under certain regularity conditions\footnote{For any window size $\sigma > 0$, the kernel $q_{\sigma}$ is differentiable, converges to 0 at infinity, and has a finite gradient norm},
the Parzen window based objective is equivalent to the original objective proposed by \cite{hyvarinen2005} in \eqnref{eq:implicit_sm}

Taking it one step further, assume the Parzen density estimate is chosen to estimate the joint density of clean and \textit{corrupted} samples $(x, \tilde{x})$ i.e. $q_{\sigma}(x, \tilde{x}) = q_{\sigma}(x | \tilde{x} ) p(x) $. Thus, the DSM objective is simply:
\begin{align}
\label{eq:denoising_sm}
    J_{DSM}(\theta) &= \mathbb{E}_{\tilde{x}\sim q_{\sigma}(x, \tilde{x})} \frac{1}{2} \s{ \norm{s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_{\sigma}(x | \tilde{x}) }^2 }
\end{align}

DSM mitigates the need for computing second order gradients as is the case for \eqnref{eq:implicit_sm}. Furthermore, if $q_{\sigma}$ is set as the Gaussian kernel $\mathcal{N}(\tilde{x} |  x,\,\sigma^{2}I)$, then $\nabla_x \log q_{\sigma}(\tilde{x}) = \frac{(x - \tilde{x})}{\sigma}$. One can now see the connection between score matching and the denoising autoencoder objective (when using a Gaussian kernel) as the score model is effectively being trained to estimate the noise that was added to the image.

I emphasize that while the \cite{vincent2011connection} and many subsequent works \cite{Song2019,song2020improved,song2020score}, use the Gaussian distribution in DSM, the proof for the validity of the objective by \cite{vincent2011connection} holds for \textit{any} differentiable noise distribution.
I will make use of this fact when deriving a score matching objective for categorical data in Chapter~\ref{ch:gnsm}.

\subsection*{Noise Conditioned Score Matching}
\label{NCSM}

\cite{Song2019} extended the DSM objective in \eqnref{eq:denoising_sm} to incorporate multiple scales $\sigma$, and train a so-called Noise Conditioned Score Network (NCSN). The authors further outlined an iterative sampling algorithm, dubbed annealed Langevin dynamics, enabling the score network to be employed as a deep generative model. Let $\{\sigma_i\}_{i=1}^L$ be a positive geometric sequence that satisfies $\frac{\sigma_1}{\sigma_2} = ... = \frac{\sigma_{L-1}}{\sigma_{L}} >  1$. NCSN is a conditional network, $s_{\theta}(x,\sigma)$, trained to jointly estimate scores for various levels of noise $\sigma_i$ such that $\forall \sigma \in \{\sigma_i\}_{i=1}^L: s_{\theta}(x,\sigma) \approx \nabla_x \log q_{\sigma}(x)$. In \cite{Song2019}, the conditioning information is explicitly provided via a one-hot vector denoting the noise level used to perturb the data. The network is then trained via a modified denoising score matching objective as shown in \eqnref{eq:ncsn_dsm}.
\begin{equation}
\label{eq:ncsn_dsm}
\frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i)
\s{\frac{1}{2} \displaystyle \text{~} \mathbb{E}_{\tilde{x}\sim q_{\sigma_i} (\tilde{x}|x) p_{\text{data}}(x)} \s{ \norm{ s_{\theta}(\tilde{x}, \sigma_i) + (\frac{\tilde{x} - x}{\sigma_i^2}) }^2_2  } }
\end{equation}

% \subsubsection{manifold hypothesis}

\subsection*{Connecting Score Matching to Diffusion Models}

In a follow-up work, \cite{song2020score} described a connection between noise-conditioned score matching and diffusion models \cite{sohl2015deep}. The connection is presented under the lens of generative modeling, and provides a framework which unifies Markov-based and continuous-time diffusion models. The key insight is that successive perturbation of a data point using a scale-dependent noise distribution (as done in NCSNs), follows a Stochastic Differential Equation (SDE). Thus, the `forward' diffusion process can be modeled as an SDE. If one has access to the scores at each time point, it is possible to construct a reverse-time SDE as proved by \cite{anderson1982reverse}. These reverse-SDEs can be numerically solved using any differential equation solver, only requiring access to the score function. The authors were able to use this formulation to generate images that surpassed the state-of-the-art generative models. This work was seminal in the development of future diffusion models.

\subsection*{Continuous-Time Score Matching}
More relevant to my research, \cite{song2020score} enabled a continuous relaxation to the discretized nature of noise conditioned score matching. Defining the noising process as a continuous forward SDE alleviates the need to predetermine the number of noise scales (as was the case for NCSN models). For generative models, this translates to faster sampling as one can control the number of gradient steps to take. For my research, it gives me the ability to observe different noise scales at test time while maintaining the advantages of using multiple noise scales during training. Namely, it forces the model to learn smoother transitions between noise scales, which helps test time generalizability.

\section{Anomaly Detection}

For this thesis, I will specifically focus on \textit{unsupervised} anomaly detection. A myriad of methods have been proposed to tackle this problem \cite{pang_deep_2021,ruff_unifying_2021}, with varying success~\cite{han2022adbench}. This section provides a summary of some methodologies relevant to my research.

\subsection{Probability-based}

Many methods incorporate some form of likelihood or probability density estimation \tempcite{ANOLIKE}. 

\subsubsection*{Density}
Perhaps the most straight-forward paradigm for detecting anomalous samples is to use the probability density estimate. The principal assumption here is that anomalies are located in low-density regions in the probability space. The principle objective is then to learn the density function representative of the typical (training) data. A trained model will be used to assign probabilities to test samples, with low probabilities signifying anomalies.


One class of such models are mixture models that estimate the parameters for a mixture of probability distributions, which are then used to assign likelihoods.
Examples include Gaussian Mixture Models (GMMs)~\cite{reynolds2009gaussian} and their deep learning counter part, Deep Autoencoding Gaussian Mixture Models (DAGMM)~\cite{zong2018deep}.

There are many deep generative models that allow for the estimation of probability densities. 

\textbf{Autoregressive} models such as Glow and PixelCNN (and their earlier counterparts, NADE and MADE) allow direct access to density estimates. However, \cite{nalisnick2018do} showed that deep likelihood models often fail to detect out-of-distribution samples.

\textbf{Normalizing flow models} and failure


Variational Autoencoders~\cite{kingma2013auto} are another branch of generative models that utilize a latent variable factorization to approximate the density. While their behaviour on out-of-distribution samples has not been systematically observed, as with deep likelihood models \tempcite{Nalisnick}, they empirically have middling performance as anomaly detectors.


ECOD~\cite{li_ecod_2022} uses a different notion of density and estimates the cumulative distribution function (CDF) for each feature in the data. It then uses the tail probabilities from each learned CDF to designate samples as anomalous.


\begin{itemize}
    \item Density 
    \item Energy
\end{itemize}

\subsection{Restoration based} 
\cite{grahamDenoisingDiffusionModels2023,wyattAnoddpmAnomalyDetection2022}


\subsection{One-Class Objectives}

Despite the name, these methods do not utilize labeled samples. Instead, they try to learn the boundary that best encapsulates the training data; treating all points outside the boundary as outliers. For example, One-Class Support Vector Machines (OC-SVMs)~\cite{ocsvm} try to find the tightest hyperplane around the dataset, while Deep Support Vector Data Descriptors (DSVDD~\cite{pmlr-v80-ruff18a}  will compute the minimal hypersphere that encloses the data. Both methods assume that inliers will fall under the margins, and consequently use the distance to the margin boundaries as a score of outlierness. 

\subsection{Out-of-Distribution Detection}
A considerable amount of effort has gone into detecting out-of-distribution (OOD) data samples, especially under the lens of classification models. Thus, there is a class of methods that augment existing classifiers to better detect OOD input samples, rather than training an unsupervised detector from scratch. Perhaps the seminal work in this area was done by \citet{Hendrycks2019}. The authors were the first to significantly highlight the domain-shift discrepancy exhibited by a range of deep learning models, and established an experimental test-bed that has served as a template for subsequent OOD work. They posited that OOD samples are likely to be assigned low probabilities and purported the thresholding of softmax probabilities from well-trained classifiers to detect in-domain samples. \cite{Liang2017} propose ODIN as a post-hoc method that utilizes a pretrained network to reliably separate OOD samples from the inlier distribution. They achieve this via a two-step procedure. First, the input image is perturbed in the gradient direction of the highest (inlier) softmax probability. Next, the softmax outputs of the classifier are scaled by a temperature, which is determined via a held out test set. While the authors report good performance, ODIN's effectiveness depends heavily on tuning the hyperparameters, namely the gradient step and the temperature. \cite{devries2018learning} improved upon \cite{Hendrycks2019} (and somewhat upon ODIN) by training networks to produce confidence estimates alongside softmax probabilities for outlier thresholding. Concurrently, \cite{Lee2018} jointly trained a GAN alongside a classifier to generate realistic OOD examples. This required an additional OOD set during training, and the resulting classifiers were unable to generalize to unseen datasets.


\subsection{Methods Pertinent to Anomaly Detection in Medical Imaging}

Often what works for natural images, is not directly translatable to medical imaging. For one, medical imaging modalities such as Magnetic Resonance Imaging (MRIs) and Computerized Tomography (CT) are three dimensional by design. Thus, models designed for 2D natural images are not applicable to medical scans. It is possible to apply these models to each 2D slice individually. \tempcite{2.5D}
However, in doing so, the model forgoes a lot of context from the interdependence of neighbouring slices. As the data is 3D, so should the model. 

% However, the downside of using 3D architectures is that we lose out on models that have been pretrained on massive natural image datasets such as ImageNet \temcite{imagenet}. Pretrained models have often learned useful priors (at minimum simple shapes such as edges) and converge faster for most downstream tasks when compared to training-from-scratch \tempcite{facebook} 


- Models require more compute and memory resources
- This makes them difficult to train as experimentation and hyperparameter search is time consuming and cost prohibitive
- Many deep learning models benefit from larger batch sizes or longer training times
 - which is difficult for memory intensive 3D models

therefore, there is an observable shift in the types of approaches that are dominant in the 3D medical domain. They are usually more straight-forward and lag a year or two behind the bleeding edge research in 2D computer vision.

The most successful anomaly detetction models are based on autoencoders.

\tempcite{BAUR} gives a thorough overview on anomaly detectors.
They note that there was no clear winner as no single method definitively outperformed the rest.

