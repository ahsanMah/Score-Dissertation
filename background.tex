\chapter{Background and Prior Work}

\section{Score Matching}

\subsection{Denoising Score Matching}

\subsection{Noise Conditioned Score Matching}

\subsection{Connecting Score Matching to Diffusion Models}
Recently, \tempcite{song 2021} described a connection between noise-conditioned score matching and diffusion models \tempcite{diffusion-jho}. The connection is presented under the lens of generative modeling, and provides a unifying framework for Markov-based and continuous-time diffusion models based on estimating the scores. 
For the purposes of this research, we focus on the relaxation of the discretized nature of noise conditioned score matching.

\subsubsection*{Continuous-Time Score Matching}


\section{Anomaly Detection}

This section provides a summary of the relevant methodologies in anomaly detection.

\subsection{Likelihood-based}
\begin{itemize}
    \item Density 
    \item Energy
    \item One-class objectives
\end{itemize}

\subsection{Restoration based}

\subsection{Out-of-Distribution Detection}
A considerable amount of effort has gone into detecting out-of-distribution (OOD) data samples, especially under the lens of classification models. Thus, there is a class of methods that augment existing classifiers to better detect OOD input samples, rather than training an unsupervised detector from scratch. Perhaps the seminal work in this area was done by \cite{Hendrycks2019}. The authors were the first to significantly highlight the domain-shift discrepancy exhibited by a range of deep learning models, and established an experimental test-bed that has served as a template for subsequent OOD work. They posited that OOD samples are likely to be assigned low probabilities and purported the thresholding of softmax probabilities from well-trained classifiers to detect in-domain samples. \cite{Liang2017} propose ODIN as a post-hoc method that utilizes a pretrained network to reliably separate OOD samples from the inlier distribution. They achieve this via a two-step procedure. First, the input image is perturbed in the gradient direction of the highest (inlier) softmax probability. Next, the softmax outputs of the classifier are scaled by a temperature, which is determined via a held out test set. While the authors report good performance, ODIN's effectiveness depends heavily on tuning the hyperparameters, namely the gradient step and the temperature. \cite{devries2018learning} improved upon \cite{Hendrycks2019} (and somewhat upon ODIN) by training networks to produce confidence estimates alongside softmax probabilities for outlier thresholding. Concurrently, \cite{Lee2018} jointly trained a GAN alongside a classifier to generate realistic OOD examples. This required an additional OOD set during training, and the resulting classifiers were unable to generalize to unseen datasets.


\subsection{Methods Common to Medical Imaging}
