\chapter{Background and Prior Work}
\label{ch:background}

\section{Score Matching}
A score is defined as the gradient of the log probability density, with respect to the data. Conceptually, a score is a vector field that points in the direction where the log density grows the most. 
\cite{hyvarinen2005} introduced score matching as a means of computing the parameters of an unnormalized probability density model. The authors proved the remarkable property that learning the score involves the gradient of the score function itself as shown in Equation~\ref{eq:implicit_sm}. Following the naming scheme used in \cite{vincent2011connection}, this objective is called Implicit Score Matching.

\begin{align}
\label{eq:implicit_sm}
    J_{ISM}(\theta) &= \mathbb{E}_{x \sim p(x)} \frac{1}{2} \s{ \norm{s_\theta(x) - \nabla_x \log p(x) }^2 } \\
    &= \mathbb{E}_{x \sim p(x)} \s{ \norm{s_\theta(x)}^2 + \sum^{d}_{i=1}{\partial{x_{i}} s(x_{i}) } }
\end{align}


\subsection*{Denoising Score Matching}

\cite{vincent2011connection} formalized a connection between denoising autoencoders and score matching, and proposed the denoising score matching (DSM) objective. The authors noted how \cite{hyvarinen2005} had suggested the possibility of an alternate score matching objective; one that was based on regressing against the data gradients of a Parzen window density estimator. This so-called Explicit Score Matching objective is shown in \eqnref{eq:explicit_sm}.

\begin{align}
\label{eq:explicit_sm}
    J_{ESM}(\theta) &= \mathbb{E}_{x \sim q_{\sigma}(x)} \frac{1}{2} \s{ \norm{s_\theta(x) - \nabla_x \log q_{\sigma}(x) }^2 }
\end{align}

\cite{vincent2011connection} proved that under certain regularity conditions\footnote{For any window size $\sigma > 0$, the kernel $q_{\sigma}$ is differentiable, converges to 0 at infinity, and has a finite gradient norm},
the Parzen window based objective is equivalent to the original objective proposed by \cite{hyvarinen2005} in \eqnref{eq:implicit_sm}

Taking it one step further, assume the Parzen density estimate is chosen to estimate the joint density of clean and \textit{corrupted} samples $(x, \tilde{x})$ i.e. $q_{\sigma}(x, \tilde{x}) = q_{\sigma}(x | \tilde{x} ) p(x) $. Thus, the DSM objective is simply:
\begin{align}
\label{eq:denoising_sm}
    J_{DSM}(\theta) &= \mathbb{E}_{\tilde{x}\sim q_{\sigma}(x, \tilde{x})} \frac{1}{2} \s{ \norm{s_\theta(\tilde{x}) - \nabla_{\tilde{x}} \log q_{\sigma}(x | \tilde{x}) }^2 }
\end{align}

DSM mitigates the need for computing second order gradients as is the case for \eqnref{eq:implicit_sm}. Furthermore, if $q_{\sigma}$ is set as the Gaussian kernel $\mathcal{N}(\tilde{x} |  x,\,\sigma^{2}I)$, then $\nabla_x \log q_{\sigma}(\tilde{x}) = \frac{(x - \tilde{x})}{\sigma}$. One can now see the connection between score matching and the denoising autoencoder objective (When using a Gaussian kernel). The score model is effectively being trained to estimate the \textit{noise} that was added to the image.

It should be emphasized that while~\cite{vincent2011connection} and many subsequent works \cite{Song2019,song2020improved,song2020score}, use the Gaussian distribution in DSM, the proof for the validity of the objective by~\cite{vincent2011connection} holds for \textit{any} differentiable noise distribution.
I will make use of this fact when deriving a score matching objective for categorical data in Chapter~\ref{ch:gnsm}.

\subsection*{Noise Conditioned Score Matching}
\label{NCSM}

\cite{Song2019} extended the DSM objective in \eqnref{eq:denoising_sm} to incorporate multiple scales $\sigma$, and train a so-called Noise Conditioned Score Network (NCSN). The authors further outlined an iterative sampling algorithm, dubbed annealed Langevin dynamics, enabling the score network to be employed as a deep generative model. Let $\{\sigma_i\}_{i=1}^L$ be a positive geometric sequence that satisfies $\frac{\sigma_1}{\sigma_2} = ... = \frac{\sigma_{L-1}}{\sigma_{L}} >  1$. NCSN is a conditional network, $s_{\theta}(x,\sigma)$, trained to jointly estimate scores for various levels of noise $\sigma_i$ such that $\forall \sigma \in \{\sigma_i\}_{i=1}^L: s_{\theta}(x,\sigma) \approx \nabla_x \log q_{\sigma}(x)$. In \cite{Song2019}, the conditioning information is explicitly provided via a one-hot vector denoting the noise level used to perturb the data. The network is then trained via a modified denoising score matching objective as shown in \eqnref{eq:ncsn_dsm}.
\begin{equation}
\label{eq:ncsn_dsm}
\frac{1}{L} \sum_{i=1}^L \lambda(\sigma_i)
\s{\frac{1}{2} \displaystyle \text{~} \mathbb{E}_{\tilde{x}\sim q_{\sigma_i} (\tilde{x}|x) p_{\text{data}}(x)} \s{ \norm{ s_{\theta}(\tilde{x}, \sigma_i) + (\frac{\tilde{x} - x}{\sigma_i^2}) }^2_2  } }
\end{equation}

% \subsubsection{manifold hypothesis}

\subsection*{Connecting Score Matching to Diffusion Models}

In a follow-up work,~\cite{song2020score} described a connection between noise-conditioned score matching and diffusion models~\cite{sohl2015deep}. The connection is presented under the lens of generative modeling, and provides a framework which unifies Markov-based and continuous-time diffusion models. The key insight is that successive perturbation of a data point using a scale-dependent noise distribution (as done in NCSNs), follows a Stochastic Differential Equation (SDE). Thus, the `forward' diffusion process can be modeled as an SDE. If one has access to the scores at each time point, it is possible to construct a reverse-time SDE as proved by~\cite{anderson1982reverse}. These reverse-SDEs can be numerically solved using any differential equation solver, only requiring access to the score function. The authors were able to use this formulation to generate images that surpassed the state-of-the-art generative models. This work was seminal in the development of future diffusion models.

\subsection*{Continuous-Time Score Matching}
More relevant to this research,~\cite{song2020score} enabled a continuous relaxation to the discretized nature of noise conditioned score matching. Defining the noising process as a continuous forward SDE alleviates the need to predetermine the number of noise scales (as was the case for NCSN models). For generative models, this translates to faster sampling as one can control the number of gradient steps to take. For my research, it gives me the ability to observe different noise scales at test time while maintaining the advantages of using multiple noise scales during training. Namely, it forces the model to learn smoother transitions between noise scales, which helps test time generalizability.

\section{Anomaly Detection}

This thesis will specifically focus on \textit{unsupervised} anomaly detection. A myriad of methods have been proposed to tackle this problem \cite{pang_deep_2021,ruff_unifying_2021}, with varying success~\cite{han2022adbench}. The following sections provide a summary of some methodologies relevant to this research.

\subsection*{Probability-based}


Perhaps the most common paradigm for detecting anomalous samples is to use the probability density estimate. The principal assumption here is that anomalies are located in low-density regions in the probability space. The objective is to learn the density function representative of the typical (training) data. A trained model is used to assign probabilities to test samples, with low probabilities signifying anomalies.

\textbf{Mixture models} estimate the parameters for a mixture of probability distributions, which are then used in conjunction to estimate likelihood of the data. Gaussian Mixture Models (GMMs)~\cite{reynolds2009gaussian} are one of the most popular examples of a mixture model, where the probability is estimated from the weighted sum of Normal distributions. ~\cite{zong2018deep} introduced Deep Autoencoding Gaussian Mixture Models (DAGMM), an unsupervised anomaly detection model based on GMMs. They combined the reconstruction objective of autoencoders with the likelihood objective of mixture models. The resulting network estimates the likelihood of the \textit{low-dimensional embedding} of a data point, as obtained by the encoder network of the autoencoder. 

Certain deep generative models also allow for the estimation of probability densities. \textbf{Autoregressive} models such as Glow and PixelCNN (and their earlier counterparts, NADE and MADE) allow direct access to density estimates. However,~\cite{nalisnick2018do} showed that deep likelihood models often fail to detect out-of-distribution samples.

\textbf{Normalizing flow models} are a flexible class of generative models. They utilize invertible transformations to project the data into the space of a predefined base distribution, such as a Standard Gaussian. Examples of deep normalizing flows are models such as RealNVP~\cite{dinh2017density} or Neural Spline Flows~\cite{NEURIPS2019_7ac71d43}, which are typically trained via the maximum likelihood objective. However, these models fail to detect outlying samples much like deep autoregressive models. The work by ~\cite{normflow_fails} investigated the failure cases of flow models. The authors analyses suggest that flow models encode the visual appearance directly, without learning any semantic content. The anomaly detection performance of flow-based models can improve if they are trained on high-level semantic representations (e.g. from a pretrained neural network) rather than the raw images themselves. 


\textbf{Variational Autoencoders}~\cite{kingma2013auto} are another branch of generative models that utilize a latent variable factorization to approximate the density. While their behaviour on out-of-distribution samples has not been systematically studied, as with deep likelihood models, they empirically show middling performance as anomaly detectors~\cite{baur2021,kascenasRoleNoiseDenoising2023}.

Lastly, many \textbf{non-deep learning} approaches are still being developed for anomaly detection. COPOD~\cite{copod} is a parameter-free, highly interpretable outlier detection algorithm based on empirical copula models. DoSE~\cite{pmlr-dose} uses Kernel Density Estimates for learning the distribution of an ensemble of data statistics. The sum of the resulting log-probabilities is used as an outlier metric. ECOD~\cite{li_ecod_2022} uses a different notion of density and estimates the cumulative distribution function (CDF) for each feature in the data. It then uses the tail probabilities from each learned CDF to designate samples as anomalous.

\subsection*{Reconstruction based}

Reconstruction models such as autoencoders train a model to reproduce the input image. These models first encode the data into a low-dimensional "bottleneck" embedding, followed by a decoding step. During inference, it is assumed that when given an anomalous sample, the model will output an anomaly-free reconstruction. One can then use the reconstruction error as the anomaly metric. Examples of such methods include vanilla autoencoders trained with a mean-squared objective~\cite{aelu2023}, denoising autoencoders that are trained to reconstruct from a noisy input~\cite{dae-kascenas22a}, and generative autoencoders to `restore` the input~\cite{grahamDenoisingDiffusionModels2023,wyattAnoddpmAnomalyDetection2022}. The latter differ from the former in that they utilize generative models such as diffusion models or VAEs to iteratively remove the anomalies from the input image (hence restoring it to an anomaly-free reconstruction).
A known drawback of all reconstruction models is the lack of specificity in their detection. As no reconstruction is pixel-perfect (especially in terms of image intensities), the output error maps have significant false-positives~\cite{baur2021}. Another drawback of autoencoders is that as their reconstruction abilities improve, their anomaly detection capabilities decrease as the models are better at reconstructing the anomalies.

\subsection*{One-Class Objectives}

Despite the name, these methods do not utilize labeled samples. Instead, they try to learn the boundary that best encapsulates the training data; treating all points outside the boundary as outliers. For example, One-Class Support Vector Machines (OC-SVMs)~\cite{ocsvm} try to find the tightest hyperplane around the dataset, while Deep Support Vector Data Descriptors (DSVDD~\cite{pmlr-v80-ruff18a}  will compute the minimal hypersphere that encloses the data. Both methods assume that inliers will fall under the margins, and consequently use the distance to the margin boundaries as a score of outlierness. 

\subsection{Out-of-Distribution Detection}
A considerable amount of effort has gone into detecting out-of-distribution (OOD) data samples, especially under the lens of classification models. Thus, there is a class of methods that augment existing classifiers to better detect OOD input samples, rather than training an unsupervised detector from scratch. Perhaps the seminal work in this area was done by \citet{Hendrycks2019}. The authors were the first to significantly highlight the domain-shift discrepancy exhibited by a range of deep learning models, and established an experimental test-bed that has served as a template for subsequent OOD work. They posited that OOD samples are likely to be assigned low probabilities and purported the thresholding of softmax probabilities from well-trained classifiers to detect in-domain samples. \cite{Liang2017} propose ODIN as a post-hoc method that utilizes a pretrained network to reliably separate OOD samples from the inlier distribution. They achieve this via a two-step procedure. First, the input image is perturbed in the gradient direction of the highest (inlier) softmax probability. Next, the softmax outputs of the classifier are scaled by a temperature, which is determined via a held out test set. While the authors report good performance, ODIN's effectiveness depends heavily on tuning the hyperparameters, namely the gradient step and the temperature. \cite{devries2018learning} improved upon \cite{Hendrycks2019} (and somewhat upon ODIN) by training networks to produce confidence estimates alongside softmax probabilities for outlier thresholding. Concurrently, \cite{Lee2018} jointly trained a GAN alongside a classifier to generate realistic OOD examples. This required an additional OOD set during training, and the resulting classifiers were unable to generalize to unseen datasets.


\section{Conclusion}
This chapter has provided a comprehensive overview of the key concepts and prior work that form the foundation for the research presented in this thesis. The discussion of score matching and its connection to denoising autoencoders, noise-conditioned score networks, and diffusion models lays the groundwork for understanding the score-based methods developed in the subsequent chapters.

The survey of anomaly detection approaches - ranging from probability-based methods like mixture models and normalizing flows, to reconstruction-based techniques like autoencoders - highlights the diversity of existing methodologies. However, it also reveals limitations in the detection capabilities of these models, with models showing inconsistent performance across tasks or simply failing to detect OOD datasets. This gap motivates the need for novel unsupervised anomaly detection frameworks that can effectively model structured, non-continuous data. The review of post-hoc out-of-distribution detection techniques provides further context for the need of an approach that is not label dependent and can generalize to models other than classifiers. 

By establishing the theoretical underpinnings and contextualizing the challenges, this chapter equips the reader with the necessary background to fully appreciate the significance and novelty of the multi-scale score matching approach introduced in the next chapter. The insights gained here will also aid in understanding how the proposed methodology can be applied to uncover scientific insights from complex datasets, as demonstrated in the case studies.

% \subsection{Methods Pertinent to Anomaly Detection in Medical Imaging}

% Often what works for natural images is not equally effective in the medical image domain. For one, medical imaging modalities such as Magnetic Resonance Imaging (MRIs) and Computerized Tomography (CT) are three dimensional by design. Thus, models designed for 2D natural images are not directly applicable to medical scans. It is possible to apply these models to each 2D slice individually, or to a group of slices in a so-called 2.5D approach~\cite{suji2023survey}. However, in doing so, the model forgoes a lot of context from the interdependence of neighbouring slices. As the data is 3D, so should the model. 

% % However, the downside of using 3D architectures is that we lose out on models that have been pretrained on massive natural image datasets such as ImageNet \temcite{imagenet}. Pretrained models have often learned useful priors (at minimum simple shapes such as edges) and converge faster for most downstream tasks when compared to training-from-scratch \tempcite{facebook} 



% - Models require more compute and memory resources
% - This makes them difficult to train as experimentation and hyperparameter search is time consuming and cost prohibitive
% - Many deep learning models benefit from larger batch sizes or longer training times
%  - which is difficult for memory intensive 3D models

% therefore, there is an observable shift in the types of approaches that are dominant in the 3D medical domain. They are usually more straight-forward and lag a year or two behind the bleeding edge research in 2D computer vision.

% The most successful anomaly detection models are based on autoencoders.

% \tempcite{BAUR} gives a thorough overview on anomaly detectors.
% They note that there was no clear winner as no single method definitively outperformed the rest.

