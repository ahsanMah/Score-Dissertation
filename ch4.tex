\chapter{Demystifying Neurodivergence via Score Estimators }
\label{ch:localizing}

Atypically developing children are diagnosed through clinical assessments that measure behavioral response. While effective in diagnosis, behavioral assessments do not provide any insight into the differences in brain morphometry of the neurodivergent cohort. Existing analyses primarily look at group wise differences of brain features (such as region volumes) between cohorts~\cite{giraultNeurodevelopmentAutismInfancy2020,hamnerPediatricBrainDevelopment2018}, or correlations of said features with a behavioral assessment~\cite{shenSubcorticalBrainDevelopment2022,brainsci12040439}. These analyses fail to produce specific causal relationships between the brain regions and the neurodivergence, as they mainly focus on correlational evidence.
Furthermore, the researchers introduce additional biases when they select regions of interest (ROIs) to be studied using their preconceived notions about the disease.

Herein lies the opportunity for a data-driven methodology to uncover atypically developing ROIs that are reflected in the data but, may be underexplored in the existing  literature. One can task a model to identify subpopulations that deviate from the typical (via outlier detection), and subsequently highlight the brain ROIs shared across the subpopulations (via localization).  MSMA, combined with its localization augmentation, naturally allows for such data exploration. This chapter outlines a process through which MSMA was utilized for deriving insights about brain MRI data.

\section{A Hypothesis Generating Tool}

The previous chapters have demonstrated MSMA's anomaly detection capabilities. At the heart of MSMA is the score-norm vector, which describes a space in which the outliers become separable from the inliers. How can one explore this space to gain further insights into the data?

We can assume that a user exploring the outputs of MSMA will be looking to answer a set of research questions. Examples of the questions may be:
\begin{itemize}
    \item Do any subpopulations cluster in the score-norm space?
    \item Can we identify atypical input features (such as brain ROIs)  that are shared by samples within a cluster?
    \item Do subpopulations correlate with any behavioral assessments?
\end{itemize}

To answer such questions, one needs to be able to link multiple sources of data: the score-norm space, the localization heatmaps, and existing metadata associated with each sample. I posit to use an interactive visualization to facilitate such a task.
Using the visualization the user should be able to observe how samples are distributed in the score-norm space. Additional views can visualize the aggregated localization heatmaps for each cluster, as well as metadata such as behavioural scores.

% The visualization will also display an aggregate of anomaly scores across brain ROIs. The goal of the visualization is to make the results interpretable by a medically informed audience. By using standardized brain parcellation schemes, the visualization should enable the user to identify the most anomalous ROIs for the population of interest, and refer to the medical literature for further analysis.

\section{Exploring a High-Dimensional Space}

It is often desirable to visualize data for the purposes of exploratory research. This poses a problem if we want to visualize the space of score-norms (as a 2D image). While the score-norms are much lower in dimensionality compared to the high resolution MRIs they are derived from, each score-norm vector is still multidimensional. For reference, all of the experiments in this chapter use $L=20$ scales, resulting in a 20-dimensional vector for each sample. One has to utilize a dimensionality reduction algorithm to reduce the space into two or three dimensions so that the points may be plotted on a grid.

This section will briefly go over dimensionality reduction algorithms and why they may not be appropriate for our analysis as they can introduce spurious clusters in the visualization. Instead, I will be advocating the use of the Self-Organizing Map (SOM) to visualize my data and will discuss its advantages.


\subsection*{Common Dimensionality Reduction Algorithms}

Conventionally, researchers have used PCA\tempcite{PCA} to select the most important components to display. While this technique is fast and well understood, it has the downside of only capturing linearly separable components as it will select orthogonal components of highest variation. Recently, two other contenders have arisen to visualize high dimensional non linear data: t-SNE and UMAP.  

% \subsection*{A Brief Discussion on t-SNE and UMAP}

t-SNE (t-distributed Stochastic Neighbor Embeddings) defines the relative distance between two points as a conditional probability. It uses an optimization process to determine a low-dimensional embedding such that the relative distance between each point in the low-dimensional space matches the distance in the high-dimensional space (using the Kullback-Leibler Divergence between the pairwise conditional probability distribution).

UMAP (Uniform Manifold Approximation and Projection) computes the low-dimensional embedding in two stages. First, it constructs a weighted graph of the k-nearest neighbours (the number of neighbors being a hyperparameter). The weights of the graphs represent probability distributions. In the second stage, it uses an iterative procedure to pull the neighbors in the graph closer together in the low-dimensional space. Additionally, it applies a repulsive force between all non-neighbours in the graph, pushing them away in the low-dimensional space. Generally, UMAP is considered to have superior runtime performance compared to t-SNE~\cite{mcinnes2020umap}

\subsection*{t-SNE and UMAP Need Not Reflect the Data}
% why are they not optimal for the task?

Both t-SNE and UMAP aim to find a low-dimensional space that reflects the distances in the original space. However, the resulting low-dimensiopnal space is highly dependant on the hyperparameters. These hyperparameters (often associated with the size of the neighbourhood considered) produce a trade-off between the preservation of local structure and preservation of global structure. Often, the algorithms are unable to preserve both~\cite{pacmap}. If local structure is emphasized, the algorithms produce spurious clusters, making their results untrustworthy. Conversely, if global structure is emphasized, the algorithms tend to produce large homogenous blobs, and fail to distinguish any underlying patterns~\cite{leiQuantifyingImpactUninformative2023}.
% This can have the unfortunate effect of the practitioner tweaking the hyperparameters until the algorithm's output plots that coincide with the practitioner's existing beliefs.


\subsection*{Self Organizing Maps as an Alternative}

The Self-Organizing Map, also known as Kohonen Networks, ~\cite{kohonen1990self} produces a two-dimensional grid-like map such that each cell on the map corresponds to a location in the data-space. Nearby cell locations on the grid are closer in the data-space compared to distal cell locations. A cell is commonly also referred to as a 'neuron'.

During training, the method randomly initializes a grid of neurons (cells). Note that the weight vector corresponding to each neuron resides in the data-space. For each data sample we find the closest neuron to the current point. This is referred to as the Best Matching Unit (BMU). The BMU's weights are updated to be closer to the data point. Additionally, the neighboring neurons (in the \textit{grid-space}) are also updated. The latter step is crucial to preserve the topological structure of the data. This procedure is repeated for a pre-defined number of iterations. Most online packages implement the batch-learning algorithm described by~\cite{kinouchi2002quick}. The authors improve convergence by initializing the neurons using eigenvectors computed from the data, and by modifying learning objective that uses the full dataset in each weight update rather than a minibatch as in the original algorithm.  I will be using the latter training algorithm.

\subsection*{SOMs Circumvent Dimensionality Reduction}

Note that SOMs operate directly in the high-dimensional data space, computing distances between the data points and the neuron prototypes in this original space. This is in contrast to t-SNE and UMAP, which first project the data into a low-dimensional space, where distance computations may not accurately reflect the true distances in the high-dimensional space. As a result, SOMs can better preserve the structure and relationships within the data, reducing the risk of distortions introduced by dimensionality reduction. The topological constraint on the grid of neurons mainly facilitates the visual organization.

\subsection*{SOMs Identify Prototypes}
It is helpful to think of SOM as a vector quantization algorithm with a spatial constraint on the codebook of vectors. Thus, each neuron can be interpreted as a prototype in the data-space. The grid-like structure of SOMs provides a natural way to organize these data prototypes. The relative positions of the neurons on the grid reflect their similarities. This spatial organization can aid in identifying patterns, clusters, or relationships within the data that may not be immediately apparent from a scattered plot produced by techniques like t-SNE or UMAP.

To elucidate how one can use SOM to gain insights about the data, recall that each sample is assigned to one neuron (the sample's BMU). If class labels are available, one can tally up the labels of all the samples that are assigned to a given neuron. We can then compute the maximally matching label for each neuron, which is often  overlayed on top of the map visualization to quickly associate the prototype with a subpopulation.
[EXAMPLE in fig]

% One can go further and analyze the hsitogram
\subsection*{SOM vs tsne for clustering}

\section{Feasibility Experiments on Benchmark Datasets }

\section{Case Study: Detecting Brain ROIs involved in Down Syndrome}

Down Syndrome (DS) is one of the most common neurodevelopmental disorder, occurring at a rate of ~1/700 live births~\cite{parker2010updated}. Studies have revealed large deviations from typical development in brain structure of children with DS, with the most pronounced deviation in the total brain volume (TBV). Further, when TBV is accounted for, studies have revealed deviations in specific regions of the brain such as the temporal lobe, cerebellum, and hippocampus~\cite{hamnerPediatricBrainDevelopment2018}. However, there remains a push for developing focused research to elucidate the nature of DS neuroanatomic phenotypes~\cite{hamnerPediatricBrainDevelopment2018}.

This case study aims to uncover brain ROIs that are associated with DS, using learned score-estimates. It will incorporate MSMA, SOMs, and Spatial-MSMA to provide a cohesive analysis of structural brain MRIs belonging to a DS cohort.

The case study will be answering the following research questions: 

	- Does the score-norm space produced by MSMA capture any neuroanatomic phenotype(s) pertinent to DS ?
 
	- Does Spatial-MSMA highlight brain ROIs that are \textit{known} in the existing research?

	- Does Spatial-MSMA highlight brain ROIs that are \textit{novel}?

\subsection*{Self-Organizing Maps of MSMA reveal a DS Phenotype}
The first research question can be answered by analyzing the ouputs of a SOM trained on the score-norm space computed via MSMA.



% vector qunatization flavored
% we are determining codebooks
% each protoype resides in the data space
