\chapter{Localizing Anomalies via Patch-based MSMA}
\label{ch:localizing}

Generic anomaly detection methods operate on the full input and make binary predictions on whether a sample is anomalous. However, they do not provide granular information on which input components led to that assessment. It is often desirable to identify the specific regions within an image that are contributing to its atypicality. Such localization allows for increased model interpretability as well as directing future investigation. 

For instance, in healthcare, the ability to interpret a model's prediction not only empowers medical practitioners to visually corroborate the identified regions of interest in diagnostic images but, also paves the way for novel insights into the disease's nature. Furthermore, localizing anomalies enables targeted diagnosis and intervention planning based on the factors contributing to the detected outlier.

Another industry where anomaly detection has critical real world value is manfucaturing~\cite{Bergmann_2019_CVPR}. Supply chain industries often need to automate defect identification to reduce operational costs and time requirements. This is often done through image-based anomaly detection via cameras. As the type of defects that can emerge in production is unknown, these systems often utilized unsupervised models to enable broad detection. Additionally,  the models are also required to output pixel level segmentations to help manufacturers design solutions to remediate the defects. Although many methods have been proposed in the recent years, this still remains an active area of research~\cite{liu2024deep}. 


In this chapter, I will expound the techniques that build the backbone of Spatial-MSMA: an extension to produce attribution maps. Note that the benefits of localization are twofold. First, it allows the possibility of detecting physical anomalies directly. For example, Spatial-MSMA has the capabilities of highlighting brain regions with lesions even though it was never trained with any labels. Second, is the ability to gain exploratory insights gained through the interpretation of the resulting heatmaps. The two benefits are orthogonal and enable an expanded usecase for MSMA.

\section{Existing Techniques for Localization}

\subsection*{Reconstruction based Approaches}
These models are trained to produce typical counterparts (so-called reconstructions) of anomalous images. The methods may take some form of a deep autoencoder~\cite{dae-kascenas22a,baur2021} or a generative model~\cite{wyattAnoddpmAnomalyDetection2022,pinaya2022fast}. At test time, the models are presumed to output an anomaly-free image, with the \textit{reconstruction error} as the metric of atypicality.

\subsection*{Feature Embedding based Approaches}
The idea is to learn feature embeddings via a deep learner trained on only the typical samples. Similar to reconstrcution based methods, the assumption at test time is that the model will produce features that are \textit{close} to the feature embeddings of training population if the sample is an inlier and away otherwise. A broad class of these methods employ the Student-Teacher architecture popularized by~\cite{bergmann2020uninformed}. In this setting, we have two models, with the student model being a ``weaker" version of the teacher achieved through distillation. It is assumed that the weaker model will fail to generalize to unseen datasets, producing a discrepancy between the teacher's features and that of the student.

\subsection*{Attribution based Approaches}
These techniques draw on the insights of interpretability research. The task is to identify features of the data that contribute to the model's output. The identified features are often assigned a score relative to their importance, as determined by the rules of the interpretation technique. Examples of such methods include SHAP, Saliency Maps, and GradCAM.


\section{A Patch-Based Approach}
The basic assumption of MSMA is that inliers will occupy distinct regions in the score-norm space. At test time, we ask the question: Does this sample belong to the inliers? MSMA consequently gives an estimate of the likelihood of a sample belonging to the inlier region. 

Up until this chapter we have looked at the data samples holistically, and considered the entire set of features available to us (e.g. all the pixels in an image). However, MSMA is also amenable for \textit{subsets} of features. For instance, we may divide an image into patches and consider the score-norms of each patch independently. Now we ask the question: Does this \textit{patch} belong to the inliers? As before, MSMA will output a likelihood estimate of a test patch belonging to the inliers, but this time considering \textit{only} the given patch location.

It is possible to naively extend MSMA to consider patches at a time is straight forward. One can decompose the image into a regular grid, and train an independent MSMA model for each grid location. One could even circumvent computational costs by training/inference could be done in parallel for each patch location. However, this approach , while straightforward, has some limitations that we have to address.

\subsection*{Image Patches are Not Independent of Each Other}
Namely, we cannot ignore \textit{spatial locality}: the notion that neighbouring image patches are highly correlated. Further, we cannot even claim that patches which are spatially apart do not depend on each other. Consider how in an image of a face, observing the patches corresponding to the left eye gives us rich information about what we may observe in a large region around this patch. This gives an impetus for a conditional model where in addition to the contents of a patch,  its position and surrounding context are also taken into account.

\subsection*{Modeling Conditional Likelihoods}

Therefore, to construct my patch-based anomaly detector, I chose to employ a conditional likelihood model. The conditioning information will be the patch position and the image context. Let $ s_p = \{s(x_p)\}_{i=1}^{L}$ be the multi-scale score tensor for a given patch $x_p$ at location $p$, belonging to the image $x$. I propose to estimate the conditional likelihood model $p(s_p | p, x)$.  As this model will be outputting likelihoods of score-norms at a given location, the model is called Spatial-MSMA.

Spatial-MSMA uses a flexible class of likelihood estimators called normalizing flows introduced in Chapter~\ref{ch:background}. The patch locations are modeled via sinusoidal positional embeddings, commonly used in Transformer models~\cite{vaswani}. In order to capture global details, the original image is passed through a convolutional network with a large receptive field. The image feature embeddings are concatenated with the positional embeddings and fed into the flow model as contextual information .

\section{Prototyping on 2D Images}

To develop my methodology, I focused on the task of segmenting anomalies in 2D images. I opted to go this route due to publicly available benchmark datasets and pretained models. This helped me refine my model architecture and compare my model's performance to alternatives.

I used the MvTec anomaly detection dataset~\cite{Bergmann_2019_CVPR}. This dataset focuses on industrial inspection and comprises of a set of defect-free training images and a test set of images with various kinds of manufacturing defects. The test set includes high resolution segmentation maps which allow us to validate the performance of the anomaly detection model. Note that the difficulty of this task stems from its unsupervised nature. At training time, the model will only see typical images and it cannot assume anything about the defects at the testing stage.

Similar to MSMA, one trains Spatial-MSMA via a two-stage training scheme. A score matching model is trained first and then kept frozen to train a likelihood model on the score norms. The difference from MSMA is that the second-stage likelihood model will be trained on score norms of \textit{patches} rather than whole images.

For training the score-matching model, I finetuned a publicly available checkpoint of a score-based diffusion model pretrained on the CIFAR-10 dataset. For the second stage, I trained a deep normalizing flow model on the score norms for each patch, where the patch size is a fixed hyper parameter. The flow model receives the positional embeddings and the feature representations of the original image as context. Once the flow model is trained, we can evaluate it on every patch location in the image in a sliding-window fashion. The resulting image will be a likelihood heatmap, which I invert to get the anomaly heatmap (negative log-likeihood).

To evaluate the anomaly detection performance of this approach, I compare my results to that of the original authors of the MvTec dataset~\cite{bergmann2020uninformed}. I compute the per-region overlap metric introduced by the same authors, which measures the overlap between connected components in the ground truth and the anomaly map for every threshold. Even without any complex hyperparameter tuning, Spatial-MSMA is able to outperform the baseline by 10\% for the same patch size on the Cables class (.741 PRO-AUC vs .671 PRO-AUC).


\section{Application to Brain MRIs}

\subsection{Tumor Detection}

\subsection{Lesion Detection}
